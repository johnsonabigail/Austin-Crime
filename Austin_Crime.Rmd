---
title: "Austin_Crime"
author: "Kevin, Elle, Abigail"
date: "5/4/2022"
output: md_document
---

```{r Loading Libraries, echo=TRUE, message=FALSE, warning=FALSE, show=FALSE}
library(tidyverse)
library(mosaic)
library(ggplot2)
library(dplyr)
library(tidyr)
library(ggmap)
library(geojsonio)
library(broom)
library(data.table)
library(rsample)
library(caret)
library(modelr)
library(knitr)
library(parallel)
library(foreach)
library(pdp)
library(rpart)
library(rpart.plot)
library(gbm)
library(randomForest)
library(glmnet)
library(kableExtra)
library(pROC)
```

# Data Visualization
## Feature Engineering and Data Loading
```{r Data Cleaning, message=FALSE, warning=FALSE, echo=FALSE}
obs <- read_csv("austin_crime.csv")

# Encode the crime target variable of interest
# 1 if "Cleared by Arrest", 0 otherwise
obs$clearance_status <- ifelse(obs$clearance_status == "Cleared by Arrest", 1, 0)

# Omit any missing rows for clearance status
obs <- filter(obs, clearance_status != "NA")
# This result in ~40k observations being dropped

# We determine the arrest rate, and append it to our zip-code level data
t1 <- obs %>% mutate(number_crimes = n_distinct(unique_key)) %>%
    group_by(zipcode) %>% 
    summarise(arrest_rate =(sum(clearance_status))/sum(number_crimes))

zip_list <- as.data.frame(t1$zipcode)
zip_list <- as.character(zip_list$`t1$zipcode`)

zip <- read_csv("austin_crime_zips.csv")
zip <- select(zip, 
              c(zipcode, population_density, median_income, median_home_value,
                prop_white,arrest_rate))
zip$arrest_rate <- scale(zip$arrest_rate)
zip$zipcode <- as.character(zip$zipcode)


tx <- geojson_read("tx_zip_geo.json", what = "sp")


## Final cleaning steps: Encoding zip as a factor with 45 levels, and dropping NA values
## Resulting in 27459 observations remaining
table <- merge(obs, zip, by = "zipcode") %>% drop_na()
table$zipcode = as.factor(table$zipcode)

## We want to set some data aside for validation at the end.. so we conduct a split..
x <- initial_split(table, prob = 0.9)
table <- training(x)
validation <- testing(x)


```

# Data Visualization
## Subsetting geojson
```{r Data Cleaning, message=FALSE, warning=FALSE, echo=FALSE}
# Subsetting our geojson such that only the zipcodes of interest are included...

# Transforming to a frame for ggplot...
tx_fortified <- tidy(tx, region = "ZCTA5CE10")
tx_sub <- setDT(tx_fortified)[id %chin% zip_list]

tx_sub <- tx_sub %>% left_join(.,zip, by= c("id"="zipcode"))
tx_sub <- na.omit(tx_sub)

```

# Data Visualization
## Visualizing
```{r Data Cleaning, message=FALSE, warning=FALSE, echo=FALSE}
options(scipen = 10000)

ggplot() + geom_polygon(data = tx_sub, aes(fill = median_income, x = long, y = lat, group = group,), color = 'black') + ggtitle("Median Income in Austin Area Zipcodes (USD)")

ggplot() + geom_polygon(data = tx_sub, aes(fill = median_home_value, x = long, y = lat, group = group,), color = 'black')+ ggtitle("Median Home Value in Austin Area Zipcodes (USD)")

ggplot() + geom_polygon(data = tx_sub, aes(fill = prop_white, x = long, y = lat, group = group,), color = 'black')+ ggtitle("Proportion of Population that is White in Austin Zipcodes")

ggplot() + geom_polygon(data = tx_sub, aes(fill = arrest_rate, x = long, y = lat, group = group,), color = 'black')+ ggtitle("Normalized Proportion of Police Reports Leading to Arrest in Austin Zipcodes")
```

# Question: Does Race play a part in the prevailing arrest rate?
## Some Visualizations...
```{r Ethnicity Effects, message=FALSE, warning=FALSE, echo=FALSE}


ggplot(data = zip) + geom_point(aes(x=prop_white, y=arrest_rate, color = population_density)) +theme_linedraw()+ ggtitle("Proportion of Residents Who are White vs. Normalized Arrest Rate")
ggplot(data=zip) + geom_point(aes(x=prop_white, y=median_home_value))+theme_linedraw()+ ggtitle("Proportion of Residents Who are White vs. Median Home Value")
ggplot(data=zip) + geom_point(aes(x=population_density, y=arrest_rate))+theme_linedraw()+ ggtitle("Normalized Arrest Rate vs ZipCode Population Density(People/Sq Mile)")


```

# Models
## First Model: Logistic Regression
```{r Logistic, message=FALSE, warning=FALSE, echo=FALSE}


rmse_frame_logit=foreach(x=1:10, .combine='rbind')%do%{

x <- initial_split(table, prob = 0.8)
crime_train <- training(x)
crime_test <- testing(x)

crime_logit <- glm(clearance_status ~ population_density + median_income + median_home_value + prop_white + arrest_rate + zipcode,
                   data = crime_train, family = binomial)

modelr::rmse(crime_logit,crime_test)
} %>% as.data.frame
validate_RMSE_logit = mean(rmse_frame_logit$V1)
## Validated RMSEout of 2.16


phat_test_crime <- predict(crime_logit, crime_test, type='response')
yhat_test_crime <- ifelse(phat_test_crime > 0.5, 1, 0)
confusion_out_logit <- table(y = crime_test$clearance_status, 
                             yhat = yhat_test_crime)
accuracy <- (confusion_out_logit[1,1] + confusion_out_logit[2,2])/sum(confusion_out_logit)



kable(table %>% group_by(zipcode) %>% summarize(count = n()) %>% rbind(data.frame(zipcode = "total", count = 17502)))
```




## Second Model: Stepwise Selection
```{r Stepwise, message=FALSE, warning=FALSE, echo=FALSE}
#crime_step <- step(crime_logit, scope=~(.)^2)

rmse_frame_step=foreach(x=1:10, .combine='rbind')%do%{

x <- initial_split(table, prob = 0.8)
crime_train <- training(x)
crime_test <- testing(x)


# stepwise function chose the following model
crime_step <- glm(clearance_status ~ population_density + median_income + median_home_value + 
    prop_white + arrest_rate + zipcode + population_density:median_income + 
    median_home_value:arrest_rate + median_income:median_home_value + 
    median_income:prop_white, data = crime_train, family = binomial)

modelr::rmse(crime_step,crime_test)
} %>% as.data.frame
validate_RMSE_step = mean(rmse_frame_step$V1)
## Validated RMSEout of 2.26

```

In terms of the types of models, we started with a baseline logistic regression model, with the specification of clearance_status on everything else. After that, using the stepwise variable selection function, we computed the best set of variables and the interaction between them which performed the best. The logistic model chose by the stepwise function is 

`clearance_status ~ population_density + median_income + median_home_value + 
    prop_white + arrest_rate + zipcode + median_home_value:arrest_rate + 
    median_income:arrest_rate + median_home_value:zipcode + median_income:prop_white + 
    prop_white:arrest_rate + arrest_rate:zipcode + population_density:prop_white + 
    population_density:median_home_value + population_density:arrest_rate`
    
## Third Model: Random Forest 
```{r Random Forest, message=FALSE, warning=FALSE, echo=FALSE}

rmse_frame_forest=foreach(x=1:10, .combine='rbind')%do%{
x = initial_split(table, prop = 0.8)
omit_train = training(x)
omit_test = testing(x)

crime_forest <- randomForest(clearance_status ~ population_density + median_income + median_home_value + prop_white + arrest_rate + zipcode,
                    data = omit_train,
                    importance = TRUE, na.action = na.omit)

modelr::rmse(crime_forest,omit_test)
} %>% as.data.frame
validate_RMSE_forest = mean(rmse_frame_forest$V1) 
## Validated RMSEout of .353
```

We repeated the process for gradient boosted models. We used Gradient Boosting models with distribution as "gaussian", the number of trees 
as 10000, shrinkage as 0.01,and with a interaction depth of 4.

## Fourth Model: Gradient Boosting
```{r Gradient Boosting, message=FALSE, warning=FALSE, echo=FALSE}

rmse_frame_gbm=foreach(x=1:10, .combine='rbind')%do%{
x = initial_split(table, prop = 0.8)
omit_train = training(x)
omit_test = testing(x)


crime_gbm <- gbm(clearance_status ~ population_density + median_income + median_home_value + prop_white + arrest_rate + zipcode,
                 data = crime_train,
                 distribution = "gaussian",
                 n.trees = 500,
                 shrinkage = 0.01, interaction.depth = 4, cv.folds = 2)

modelr::rmse(crime_gbm,omit_test)
} %>% as.data.frame
validate_RMSE_gbm = mean(rmse_frame_gbm$V1) 
## RMSEout of .351

```

## Fifth Model: Lasso Model
```{r Lasso, message=FALSE, warning=FALSE, echo=FALSE}
x <- model.matrix(clearance_status ~ population_density + median_income + median_home_value + prop_white + arrest_rate + as.factor(zipcode),
                  data = table)[,-1] 
#[, -1] excludes first col (intercept)

x <- scale(x, center = TRUE, scale = TRUE) 
y <- table$clearance_status %>% as.factor()
grid <- 10^seq(10, -2, length = 100)
lasso.mod <- glmnet(x, y, alpha = 1, lambda = grid, family = "binomial")
cv.out <- cv.glmnet(x, y, alpha = 1, family = "binomial", folds=20)
bestlam <- cv.out$lambda.min

plot(lasso.mod)
title("Figure 1: Lasso Coefficients as a Function of L1 Norm", line = 3)

```

```{r Lasso2, warning=FALSE, echo=FALSE}
plot(cv.out)
title("Figure 2: Mean-Squared Error as a Function of log Lambda", line = 3)
```

```{r Lasso3, warning=FALSE, echo=FALSE}
table <- merge(obs, zip, by = "zipcode") %>% drop_na()
table$zipcode = as.factor(table$zipcode)

lasso.coef <- predict(lasso.mod,
                      type = "coefficients",
                      s = bestlam)

LassoCoef <- as.data.table(as.matrix(lasso.coef), keep.rownames = TRUE)
kable(LassoCoef, col.names = c("Predictor", "Estimate"), caption = "**Table 1 Lasso Model Predictor Estimates**", format_caption = c("italic", "underline")) %>%
    kable_styling(bootstrap_options = "striped", full_width = F)


mse = min(cv.out$cvm)
rmse = sqrt(mse)
# .9014


```


## Confusion Matrix for the best model...
```{r Validation_GBM, message=FALSE, warning=FALSE, echo=FALSE}

pred =predict(crime_gbm,newdata=validation)
confusion_table = data.frame(fold_id=integer(),TPR=integer(),FPR=integer())

level = seq(.05,.5,by=.05)

confusion_level=foreach(x=level)%do%{
yhat_test = ifelse(pred > x,1,0)
confusion_out = table(y=validation$clearance_status, yhat = yhat_test)
TPR = (confusion_out[2,2]/(confusion_out[2,1]+confusion_out[2,2]))
FPR = (confusion_out[1,2]/(confusion_out[1,1]+confusion_out[1,2]))
confusion_table[nrow(confusion_table)+1,] = c(x,TPR,FPR)
}


confusion_table %>% ggplot(aes(FPR,TPR)) + geom_line(fill="steelblue") + labs(y= "True Positive Rate", x="False Positive Rate", title = "ROC Curve for GBM Model")+theme_linedraw() + geom_abline(slope=1,intercept = 0)

gbm_auc = auc(validation$clearance_status,pred)
## .669
```
## Confusion Matrix for the second best model...
```{r Validation_RF, message=FALSE, warning=FALSE, echo=FALSE}

pred =predict(crime_forest,newdata=validation)
confusion_table = data.frame(fold_id=integer(),TPR=integer(),FPR=integer())

level = seq(.05,.5,by=.05)

confusion_level=foreach(x=level)%do%{
yhat_test = ifelse(pred > x,1,0)
confusion_out = table(y=validation$clearance_status, yhat = yhat_test)
TPR = (confusion_out[2,2]/(confusion_out[2,1]+confusion_out[2,2]))
FPR = (confusion_out[1,2]/(confusion_out[1,1]+confusion_out[1,2]))
confusion_table[nrow(confusion_table)+1,] = c(x,TPR,FPR)
}


confusion_table %>% ggplot(aes(FPR,TPR)) + geom_line(fill="steelblue") + labs(y= "True Positive Rate", x="False Positive Rate", title = "ROC Curve for Random Forest Model")+theme_linedraw() + geom_abline(slope=1,intercept = 0)


forest_auc = auc(validation$clearance_status,pred)
## .667

```


```{r Validation_RF, message=FALSE, warning=FALSE, echo=FALSE}
## Variable Importance Plots

vi = varImpPlot(crime_forest, type=1)

omit_test = as.data.frame(table)
partialPlot(crime_forest, table, 'prop_white')
partialPlot(crime_forest, table, 'median_home_value')
partialPlot(crime_forest, table, 'median_income')
partialPlot(crime_forest, table, 'population_density')

```