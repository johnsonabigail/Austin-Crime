---
title: "Austin_Crime"
author: "Kevin, Elle, Abigail"
date: "5/4/2022"
output: md_document
---

```{r Loading Libraries, echo=TRUE, message=FALSE, warning=FALSE, show=FALSE}

library(tidyverse)
library(mosaic)
library(ggplot2)
library(dplyr)
library(stringr)
library(tidyr)
library(ggmap)
library(geojsonio)
library(broom)
library(data.table)
library(rsample)
library(caret)
library(modelr)
library(knitr)
library(parallel)
library(foreach)
```

# Data Visualization
## Feature Engineering and Data Loading
```{r Data Cleaning, message=FALSE, warning=FALSE, echo=FALSE}
obs <- read_csv("austin_crime.csv")

# Encode the crime target variable of interest
# 1 if "Cleared by Arrest", 0 otherwise
obs$clearance_status <- ifelse(obs$clearance_status == "Cleared by Arrest", 1, 0)

# Omit any missing rows for clearance status
obs <- filter(obs, clearance_status != "NA")
# This result in ~40k observations being dropped

# We determine the arrest rate, and append it to our zip-code level data
t1 <- obs %>% mutate(number_crimes = n_distinct(unique_key)) %>%
    group_by(zipcode) %>% 
    summarise(arrest_rate =(sum(clearance_status))/sum(number_crimes))

zip_list <- as.data.frame(t1$zipcode)
zip_list <- as.character(zip_list$`t1$zipcode`)

zip <- read_csv("austin_crime_zips.csv")
zip <- select(zip, 
              c(zipcode, population_density, median_income, median_home_value,
                prop_white,arrest_rate))
zip$arrest_rate <- scale(zip$arrest_rate)
zip$zipcode <- as.character(zip$zipcode)


tx <- geojson_read("tx_zip_geo.json", what = "sp")

```

# Data Visualization
## Subsetting geojson
```{r Data Cleaning, message=FALSE, warning=FALSE, echo=FALSE}
# Subsetting our geojson such that only the zipcodes of interest are included...

# Transforming to a frame for ggplot...
tx_fortified <- tidy(tx, region = "ZCTA5CE10")
tx_sub <- setDT(tx_fortified)[id %chin% zip_list]

tx_sub <- tx_sub %>% left_join(.,zip, by= c("id"="zipcode"))
tx_sub <- na.omit(tx_sub)

```

# Data Visualization
## Visualizing
```{r Data Cleaning, message=FALSE, warning=FALSE, echo=FALSE}
options(scipen = 10000)

ggplot() + geom_polygon(data = tx_sub, aes(fill = median_income, x = long, y = lat, group = group,), color = 'black')

ggplot() + geom_polygon(data = tx_sub, aes(fill = median_home_value, x = long, y = lat, group = group,), color = 'black')

ggplot() + geom_polygon(data = tx_sub, aes(fill = prop_white, x = long, y = lat, group = group,), color = 'black')

ggplot() + geom_polygon(data = tx_sub, aes(fill = arrest_rate, x = long, y = lat, group = group,), color = 'black')
```

# Models
## First Model: Logistic Regression
```{r Logistic, message=FALSE, warning=FALSE, echo=FALSE}
table <- merge(obs, zip, by = "zipcode")

crime <- initial_split(table, prob = 0.8)
crime_train <- training(crime)
crime_test <- testing(crime)

crime_logit <- glm(clearance_status ~ population_density + median_income + median_home_value + prop_white + arrest_rate + zipcode,
                   data = crime_train, family = binomial)

phat_test_crime <- predict(crime_logit, crime_test, type='response')
yhat_test_crime <- ifelse(phat_test_crime > 0.5, 1, 0)
confusion_out_logit <- table(y = crime_test$clearance_status, 
                             yhat = yhat_test_crime)
accuracy <- (confusion_out_logit[1,1] + confusion_out_logit[2,2])/sum(confusion_out_logit)

rmse(crime_logit, crime_test)

kable(table %>% group_by(zipcode) %>% summarize(count = n()) %>% rbind(data.frame(zipcode = "total", count = 17502)))
```

## Variable Selection stepwise
```{r Stepwise, message=FALSE, warning=FALSE, echo=FALSE}
# crime_step <- step(crime_logit, scope=~(.)^2)

# stepwise function chose the following model
crime_step <- glm(clearance_status ~ population_density + median_income + median_home_value + 
    prop_white + arrest_rate + zipcode + population_density:median_income + 
    median_home_value:arrest_rate + median_income:median_home_value + 
    median_income:prop_white, data = crime_train, family = binomial)

```

In terms of the types of models, we started with a baseline logistic regression model, with the specification of clearance_status on everything else. After that, using the stepwise variable selection function, we computed the best set of variables and the interaction between them which performed the best. The logistic model chose by the stepwise function is 

`clearance_status ~ population_density + median_income + median_home_value + 
    prop_white + arrest_rate + zipcode + population_density:median_income + 
    median_home_value:arrest_rate + median_income:median_home_value + 
    median_income:prop_white, data = crime_train, family = binomial)`
    
## Second Model: Tree
```{r Tree, message=FALSE, warning=FALSE, echo=FALSE}
trctrl <- trainControl(method = "cv", number = 5, savePredictions=TRUE)

crime_tree1 <- train(clearance_status ~ population_density + median_income + median_home_value + prop_white + arrest_rate + zipcode,
                     data = crime_train,
                     method = "rpart",
                     trControl = trctrl,
                     tuneLength = 0)

crime_tree1$results$RMSE

crime.tree <- rpart(clearance_status ~ population_density + median_income + median_home_value + prop_white + arrest_rate + zipcode, 
                    data = crime_train,
                    control = rpart.control(cp = 0.002, minsplit=30))

prune_1se = function(my_tree) {
  out = as.data.frame(my_tree$cptable)
  thresh = min(out$xerror + out$xstd)
  cp_opt = max(out$CP[out$xerror <= thresh])
  prune(my_tree, cp=cp_opt)
}

crime.prune <- prune_1se(crime.tree)
```

## Third Model: Random Forest
```{r Random Forest, message=FALSE, warning=FALSE, echo=FALSE}
trctrl <- trainControl(method = "cv", number = 5, savePredictions=TRUE)
crime_forest1 <- train(clearance_status ~ population_density + median_income + median_home_value + prop_white + arrest_rate + zipcode,
                       data = crime_train,
                       method = "rf", trControl=trctrl,
                       prox=TRUE, tuneLength=1)

crime_forest1$results$RMSE

crime.forest <- randomForest(clearance_status ~ population_density + median_income + median_home_value + prop_white + arrest_rate + zipcode,
                    data = green_train,
                    importance = TRUE, na.action = na.omit)
```


## Fifth Model: Lasso Model
```{r Lasso, message=FALSE, warning=FALSE, echo=FALSE}
x <- model.matrix(clearance_status ~ population_density + median_income + median_home_value + prop_white + arrest_rate + zipcode,
                  data = table)[,-1] 
#[, -1] excludes first col (intercept)

x <- scale(x, center = TRUE, scale = TRUE) 
y <- table$clearance_status
grid <- 10^seq(10, -2, length = 100)
lasso.mod <- glmnet(x, y, alpha = 1, lambda = grid)
cv.out <- cv.glmnet(x, y, alpha = 1)
bestlam <- cv.out$lambda.min

plot(lasso.mod)
title("Figure 1: Lasso Coefficients as a Function of L1 Norm", line = 3)
